{"cells":[{"metadata":{},"cell_type":"markdown","source":"# helper funcs from https://www.kaggle.com/its7171/metrics-evaluation-script and https://www.kaggle.com/hocop1/centernet-baseline with some change and some constants"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom math import sqrt, acos, pi, sin, cos\nfrom scipy.spatial.transform import Rotation as R\nfrom sklearn.metrics import average_precision_score\nfrom multiprocessing import Pool\nimport cv2\nimport matplotlib.pyplot as plt\nfrom math import sin, cos, sqrt\n\nsubmission_path_name1 = '../input/center-efficientnet-submission/predictions.csv'\nsubmission_path_name2 = '../input/masked-centernet-baseline-submisstion/predictions.csv'\ntest_images_dir = '../input/pku-autonomous-driving/test_images/'\nTD_thr = 0.05\nRD_thr = 10\n\n# the real threshold is (5400/distance)*ID_thr_factor, where distance is the distance from the camera to a current object.\nID_thr_factor = 0.3\nuse_ID_thr = True\n\n#merging_plan = 'non-max-suppress'\nmerging_plan = 'weighted-add'\n\n# From camera.zip\ncamera_matrix = np.array([[2304.5479, 0,  1686.2379],\n                          [0, 2305.8757, 1354.9849],\n                          [0, 0, 1]], dtype=np.float32)\n\ndef expand_df(df, PredictionStringCols):\n    df = df.dropna().copy()\n    df['NumCars'] = [int((x.count(' ')+1)/7) for x in df['PredictionString']]\n\n    image_id_expanded = [item for item, count in zip(df['ImageId'], df['NumCars']) for i in range(count)]\n    prediction_strings_expanded = df['PredictionString'].str.split(' ',expand = True).values.reshape(-1,7).astype(float)\n    prediction_strings_expanded = prediction_strings_expanded[~np.isnan(prediction_strings_expanded).all(axis=1)]\n    df = pd.DataFrame(\n        {\n            'ImageId': image_id_expanded,\n            PredictionStringCols[0]:prediction_strings_expanded[:,0],\n            PredictionStringCols[1]:prediction_strings_expanded[:,1],\n            PredictionStringCols[2]:prediction_strings_expanded[:,2],\n            PredictionStringCols[3]:prediction_strings_expanded[:,3],\n            PredictionStringCols[4]:prediction_strings_expanded[:,4],\n            PredictionStringCols[5]:prediction_strings_expanded[:,5],\n            PredictionStringCols[6]:prediction_strings_expanded[:,6]\n        })\n    return df\n\ndef str2coords(s, names=['yaw', 'pitch', 'roll', 'x', 'y', 'z', 'confidence']):\n    coords = []\n    for l in np.array(s.split()).reshape([-1, 7]):\n        coords.append(dict(zip(names, l.astype('float'))))\n    return coords\n\ndef TranslationDistance(p,g, abs_dist = False):\n    dx = p['x'] - g['x']\n    dy = p['y'] - g['y']\n    dz = p['z'] - g['z']\n    diff0 = (g['x']**2 + g['y']**2 + g['z']**2)**0.5\n    diff1 = (dx**2 + dy**2 + dz**2)**0.5\n    if abs_dist:\n        diff = diff1\n    else:\n        diff = diff1/diff0\n    return diff\n\ndef RotationDistance(p, g):\n    true=[ g['pitch'] ,g['yaw'] ,g['roll'] ]\n    pred=[ p['pitch'] ,p['yaw'] ,p['roll'] ]\n    q1 = R.from_euler('xyz', true)\n    q2 = R.from_euler('xyz', pred)\n    diff = R.inv(q2) * q1\n    W = np.clip(diff.as_quat()[-1], -1., 1.)\n    \n    # in the official metrics code:\n    # https://www.kaggle.com/c/pku-autonomous-driving/overview/evaluation\n    #   return Object3D.RadianToDegree( Math.Acos(diff.W) )\n    # this code treat θ and θ+2π differntly.\n    # So this should be fixed as follows.\n    W = (acos(W)*360)/pi\n    if W > 180:\n        W = 360 - W\n    return W\n\ndef coords2str(coords, names=['yaw', 'pitch', 'roll', 'x', 'y', 'z', 'confidence']):\n    s = []\n    for c in coords:\n        for n in names:\n            s.append(str(c.get(n, 0)))\n    return ' '.join(s)\n\ndef imread(path, fast_mode=False):\n    img = cv2.imread(path)\n    if not fast_mode and img is not None and len(img.shape) == 3:\n        img = np.array(img[:, :, ::-1])\n    return img\n\ndef get_img_coords(inp):\n    '''\n    Input is a PredictionString (e.g. from train dataframe) or a coordinate list\n    Output is two arrays:\n        xs: x coordinates in the image (row)\n        ys: y coordinates in the image (column)\n    '''\n    if isinstance(inp, str):\n        coords = str2coords(inp)\n    else:\n        coords = inp\n    xs = [c['x'] for c in coords]\n    ys = [c['y'] for c in coords]\n    zs = [c['z'] for c in coords]\n    P = np.array(list(zip(xs, ys, zs))).T\n    img_p = np.dot(camera_matrix, P).T\n    img_p[:, 0] /= img_p[:, 2]\n    img_p[:, 1] /= img_p[:, 2]\n    img_xs = img_p[:, 0]\n    img_ys = img_p[:, 1]\n    img_zs = img_p[:, 2] # z = Distance from the camera\n    return img_xs, img_ys\n\n# convert euler angle to rotation matrix\ndef euler_to_Rot(yaw, pitch, roll):\n    Y = np.array([[cos(yaw), 0, sin(yaw)],\n                  [0, 1, 0],\n                  [-sin(yaw), 0, cos(yaw)]])\n    P = np.array([[1, 0, 0],\n                  [0, cos(pitch), -sin(pitch)],\n                  [0, sin(pitch), cos(pitch)]])\n    R = np.array([[cos(roll), -sin(roll), 0],\n                  [sin(roll), cos(roll), 0],\n                  [0, 0, 1]])\n    return np.dot(Y, np.dot(P, R))\n\ndef draw_line(image, points):\n    color = (255, 0, 0)\n    cv2.line(image, tuple(points[0][:2]), tuple(points[3][:2]), color, 16)\n    cv2.line(image, tuple(points[0][:2]), tuple(points[1][:2]), color, 16)\n    cv2.line(image, tuple(points[1][:2]), tuple(points[2][:2]), color, 16)\n    cv2.line(image, tuple(points[2][:2]), tuple(points[3][:2]), color, 16)\n    return image\n\n\ndef draw_points(image, points):\n    for (p_x, p_y, p_z) in points:\n        cv2.circle(image, (p_x, p_y), int(1000 / p_z), (0, 255, 0), -1)\n#         if p_x > image.shape[1] or p_y > image.shape[0]:\n#             print('Point', p_x, p_y, 'is out of image with shape', image.shape)\n    return image\n\ndef visualize(img, coords):\n    # You will also need functions from the previous cells\n    x_l = 1.02\n    y_l = 0.80\n    z_l = 2.31\n    \n    img = img.copy()\n    for point in coords:\n        # Get values\n        x, y, z = point['x'], point['y'], point['z']\n        yaw, pitch, roll = -point['pitch'], -point['yaw'], -point['roll']\n        # Math\n        Rt = np.eye(4)\n        t = np.array([x, y, z])\n        Rt[:3, 3] = t\n        Rt[:3, :3] = euler_to_Rot(yaw, pitch, roll).T\n        Rt = Rt[:3, :]\n        P = np.array([[x_l, -y_l, -z_l, 1],\n                      [x_l, -y_l, z_l, 1],\n                      [-x_l, -y_l, z_l, 1],\n                      [-x_l, -y_l, -z_l, 1],\n                      [0, 0, 0, 1]]).T\n        img_cor_points = np.dot(camera_matrix, np.dot(Rt, P))\n        img_cor_points = img_cor_points.T\n        img_cor_points[:, 0] /= img_cor_points[:, 2]\n        img_cor_points[:, 1] /= img_cor_points[:, 2]\n        img_cor_points = img_cor_points.astype(int)\n        # Drawing\n        img = draw_line(img, img_cor_points)\n        img = draw_points(img, img_cor_points[-1:])\n    \n    return img","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if not use_ID_thr:\n    def is_close_enough(a, b, TD_thr=TD_thr, RD_thr=RD_thr):\n        # judge if a and b are close enough.\n\n        RD = RotationDistance(a, b)\n        if RD > RD_thr:\n            return False, None, None\n        TD = TranslationDistance(a, b)\n        if TD > TD_thr:\n            return False, None, None\n        return True, TD, RD\nelse:\n    # use image coordinates\n    def is_close_enough(a, b, thr_factor=ID_thr_factor):\n        # judge if a and b are close enough.\n\n        RD = RotationDistance(a, b)\n        if RD > RD_thr:\n            return False, None, None\n\n        img_x_a, img_y_a = get_img_coords([a])\n        img_x_b, img_y_b = get_img_coords([b])\n        distance = sqrt((img_x_a-img_x_b)*(img_x_a-img_x_b)+(img_y_a-img_y_b)*(img_y_a-img_y_b))\n        #print(distance)\n\n        if distance > (5400.0/a['z'])*thr_factor:\n            return False, None, None\n\n        return True, distance, None","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# merging"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"submission1 = pd.read_csv(submission_path_name1)\nsubmission2 = pd.read_csv(submission_path_name2)\n\n# to avoid length difference\nif len(submission1) > len(submission2):\n    main_submission = submission1\n    auxiliary_submission = submission2\nelse:\n    main_submission = submission2\n    auxiliary_submission = submission1\n\nnew_submission = main_submission.copy()    \nfor i in range(len(auxiliary_submission)):\n    AS_predictions = auxiliary_submission.loc[i, 'PredictionString']\n    #AS_predictions = auxiliary_submission.loc[auxiliary_submission['ImageId']=='ID_001c3cc69', 'PredictionString']\n    if isinstance(AS_predictions, float):\n        A_coords = []\n    else:\n        A_coords = str2coords(AS_predictions)\n        \n    MS_predictions = main_submission.loc[main_submission['ImageId']==auxiliary_submission.loc[i,'ImageId'], 'PredictionString'].values[0]\n    #MS_predictions = main_submission.loc[main_submission['ImageId']=='ID_001c3cc69', 'PredictionString']\n    if isinstance(MS_predictions, float):\n        M_coords = []\n    else:\n        M_coords = str2coords(MS_predictions)\n        \n    new_M_coords = M_coords.copy()\n    paired_M_coords = []\n    for A_coord in A_coords:\n        #print('')\n        candidate = {'candidate':None, 'TD':99999999}\n        for j in range(len(M_coords)):\n            M_coord = M_coords[j]\n            enough, TD, RD = is_close_enough(A_coord, M_coord)\n            #print(enough)\n            if enough and M_coord not in paired_M_coords and candidate['TD']>TD:\n                candidate['candidate'] = M_coord\n                candidate['TD'] = TD\n                candidate['index'] = j\n        if candidate['candidate'] == None:\n            new_M_coords.append(A_coord)\n        else:\n            M_coord = candidate['candidate']\n            paired_M_coords.append(M_coord)\n            if merging_plan == 'weighted-add':\n                new_coord = {}\n                for item in ['pitch','yaw','roll','x','y','z']:\n                    total_score = A_coord['confidence'] + M_coord['confidence']\n                    new_coord[item] = (A_coord[item]*(A_coord['confidence']/total_score))\\\n                                    + (M_coord[item]*(M_coord['confidence']/total_score))       \n            else:\n                if A_coord['confidence'] > M_coord['confidence']:\n                    new_coord = A_coord\n                else:\n                    new_coord = M_coord\n            new_coord['confidence'] = 1 - ((1-A_coord['confidence'])*(1-M_coord['confidence']))\n            new_M_coords[candidate['index']] = new_coord\n    new_submission.loc[new_submission['ImageId']==auxiliary_submission.loc[i,'ImageId'], 'PredictionString'] = coords2str(new_M_coords)\n    \nnew_submission.to_csv('./new_submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# let's check the output"},{"metadata":{"trusted":true},"cell_type":"code","source":"for idx in range(20):\n    \n    img = imread(test_images_dir+new_submission['ImageId'].iloc[idx]+'.jpg')\n    string1 = submission1.loc[submission1['ImageId']==new_submission['ImageId'].iloc[idx], 'PredictionString'].values[0]\n    if isinstance(string1, float):\n        string1 = ''\n    string2 = submission2.loc[submission2['ImageId']==new_submission['ImageId'].iloc[idx], 'PredictionString'].values[0]\n    if isinstance(string2, float):\n        string2 = ''\n    new_string = new_submission['PredictionString'].iloc[idx]\n    if isinstance(new_string, float):\n        new_string = ''\n        \n    coords1 = str2coords(string1)\n    coords2 = str2coords(string2)\n    new_coords = str2coords(new_string)\n    \n    fig, axes = plt.subplots(1, 3, figsize=(30,30))\n    axes[0].set_title('Prediction 1')\n    axes[0].imshow(visualize(img, coords1))\n    axes[1].set_title('Prediction 2')\n    axes[1].imshow(visualize(img, coords2))\n    axes[2].set_title('New Prediction')\n    axes[2].imshow(visualize(img, new_coords))\n    plt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}